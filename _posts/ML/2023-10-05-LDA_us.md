---
layout: single
title: "Unsupervised Linear Discriminant Analysis (LDA)"
categories: ML
tag: [ML]
author_profile: false
search: true
use_tex: true
---

> 비지도 LDA(Unsupervised LDA) 또는 비지도 판별 분석은 클래스 레이블 정보 없이 데이터셋의 판별 정보를 찾는 방법을 탐색한다. 
> 이는 데이터의 구조를 이해하거나 차원을 축소하는 데 사용될 수 있다. 
> 비지도 LDA 는 데이터의 내재된 구조를 발견하려고 시도하며, 이는 주로 데이터의 클러스터링을 통해 이루어진다.

# Linear Discriminant Analysis (LDA)

> It is basement on classification in supervised learning.
> Please refer to LDA page in supervised learning, classification.                                                                              
> [Linear Discriminant Analysis (LDA)]({{site.url}}/ml/LDA)


# LDA

- Discriminant function $p_k(x)$ 에 대한 results 를 return 하는 함수.

$$
P(Y=k|X=x)= \frac{\blue{P(X=x|Y=k) \ \orange{P(Y=k)}}}{\sum^K_{l=i} \blue{P(X=x|Y=l)} \ \orange{P(Y=l)}} \\ = \frac{\orange{\pi_k}\ \blue{f_k(x)}}{\sum^K_{l=1} \orange{\pi_l} \ \blue{f_l(x)}}
$$

$$
arg \underset{1\leq k \leq K }{\textrm{max}} P(Y=k|X=x) \\ = arg \underset{1\leq k \leq K }{\textrm{max}}\pi_k\ \textrm{exp}(-\frac{1}{2\sigma^2}(x^2-2\mu_kx+\mu^2_k)) \\ = arg \underset{1\leq k \leq K }{\textrm{max}}\pi_k\ \textrm{exp}(-\frac{1}{2\sigma^2}(x^2-2\mu_kx+\mu^2_k)+\textrm{log}(\pi_k)) \\ = arg \underset{1\leq k \leq K }{\textrm{max}}\orange{ x \cdot \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2} +\textrm{log}(\pi_k) }
$$

- Boundary formation 은 $\sigma_1(x)=\sigma_2(x)$ 를 통해 얻을 수 있다.

![Screenshot 2023-03-09 at 12.35.24 PM.png](Linear%20Discriminant%20Analysis%20(LDA)%202b14db1bad49487b85eb708d31b1c1bf/Screenshot_2023-03-09_at_12.35.24_PM.png)

# Dimension Reduction with LDA

PCA is unsupervised method. 데이터의 전체적인 variance 을 refer 해 find out principal componant, 새로운 feature 로 data projection 하는 것이 목적

LDA is methodology to find an axis in a supervised way and project the data

이전의 method 는 Bayes’ rule’s likelihood model (확률 모형) 으로부터 figure out 된 way.

View to variance, LDA 를 똑같이 똑같이 figure-out. Useable in dimension reduction.

![Screenshot 2023-03-15 at 12.37.02 AM.png](Linear%20Discriminant%20Analysis%20(LDA)%202b14db1bad49487b85eb708d31b1c1bf/Screenshot_2023-03-15_at_12.37.02_AM.png)

데이터의 dimension reduction 후에, data in same class 안에서의 variance (within variance) 는 minimize, another class 에서의 variance (between variance) 는 maximize.

Number of class : 2, 2-dimensional data, x 를 previous purpose 를 만족하는 unit vector, w 로 project

Project data x to w, 1-dimensional 에서의 coordinate, p 로 정의.

- $p_i=w^Tx_i$

각각의 class’ center vactor $m_1,m_2$ 정의

- $m_1=\frac1{N_1}\sum_{y_i\in C_1}x_i, m_2=\frac1{N_2}\sum_{y_i\in C_2}x_i$

Center vector 또한, unit vector, w 로 projection

- $\overline p_k=w^Tm_k$

Within variance

$$
s^2_1+s^2_2=\sum_{y_i\in C_1}(p_i-\overline p_1)^2+\sum_{y_i\in C_2}(p_i-\overline p_2)^2
$$

Between variance

$$
\sum_{k \in \ 1,2}(\overline p_k-\frac{\overline p_1+\overline p_2}{2})^2 \approx(\overline p_1-\overline p_2)^2
$$

위 두 variance 를 이용한 object function (목적식) $J(w)$

$$
j(w)=\frac{(\overline p_1-\overline p_2)^2}{s^2_1+s^2_2}\\ =\frac{(\overline p_1-\overline p_2)^2}{\sum_{y_i\in C_1}(p_i-\overline p_1)^2+\sum_{y_i\in C_2}(p_i-\overline p_2)^2} \\= \frac{\orange{w^T}(m_1-m_2)(m_1-m_2)^T\orange w}{\orange{w^T}(\sum_{y_i\in C_1}(x_i-m_1)(x_i-m_1)^T+\sum_{y_i\in C_2}x_i-m_2)(x_i-m_2)^T)\orange w} \\ \therefore J(w)= \frac{w^T\orange{S_B}w}{w^T\orange{S_W}w}
$$

Object function 을 Differential 할 때, 0 이 되는 지점에서 maximimum

$$
2S_BW(W^TS_WW)-(w^TS_Bw)2S_ww=0 \\ \Rightarrow S_Bw-\frac{w^TS_Bw}{w^TS_Ww}s_ww=0 \\ \Rightarrow S_Bw-\orange{J(w)}S_Ww=0
$$

- $S_Bw-J(w)S_Ww=0$
- $(S^{-1}_WS_B)w=J(w)w$

결국, object function 을 maximize 하는 direction vector는 $S^{-1}_WS_B$ matrix 의 first eigenvector 이다.

Eigenvalue 가 maximum 일 때, eigenvector 로 data projection 하여 demension reduction.

이 때의 eigenvector 는 likelihood model 을 통해 얻은 boundary of decision 과 related in perpendicular.
