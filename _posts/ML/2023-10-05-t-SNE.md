---
layout: single
title: "t-SNE"
categories: ML
tag: [ML]
author_profile: false
search: true
use_tex: true
---

> t-SNE(t-Distributed Stochastic Neighbor Embedding)는 고차원 데이터의 구조를 보존하면서 데이터를 저차원(주로 2D 또는 3D)으로 매핑하는 비선형 차원 축소 기법이다.
> 이 알고리즘은 유사한 데이터 포인트들이 저차원에서도 가까이 위치하도록 하며, 원본 고차원 공간의 데이터 포인트 간 유사도를 t-분포를 사용하여 저차원 공간에 재현함으로써, 데이터의 클러스터 구조를 시각적으로 해석하기 용이하게 만든다.

# SNE (Stochastic Neighbor Embedding)

---

---

<span style='color:#3399ff'>At present dimension state S</span>, 에서의 데이터에 대한 distance 정보를, <span style='color:#66cc66'>dimension reduction 된 state S’</span> 에서도 유지시키는 목적

Data Sample $x_i$ 를 기준으로 다른 샘플들에 대해 <span style='color:#ff7fff'>Gaussian distribution</span> 를 활용해 <span style='color:orange'>**weight (distance)**</span> 부여

$$
\blue{p_{j|i}}=\frac{e^{-\frac{||\blue{x_i-x_j}||^2}{2\sigma^2_i}}}{\sum_{k\neq i}e^{-\frac{||\blue{x_i-x_k}||^2}{2\sigma^2_i}}}, \ \ \green{q_{j|i}}=\frac{e^{-||\green{y_i-y_j}||^2}}{\sum_{k\neq i}e^{-||\green{y_i-y_k}||^2}}
$$

Each samples 마다 다른 sample과의 distance 에 대한 distribution 은 매우 다르다. ($\sigma_i$ 사용)

원형 데이터에서의 distance와 reducted dimension 에서의 distance 를, 비슷하게 만들어 주는 define the loss function

$$
L=\sum_iKL(\blue{P_i}||\green{Q_i})=\sum_i\sum_j\blue{p_{j|i}}\log\frac{\blue{p_{j|i}}}{\green{q_{j|i}}}
$$

Distance of between distribution’s 를 quantization (정량화) 하는 **KL divergence** (kullback leibler divergence) 를 사용한다.

Gradient descent method 를 이용해 $y_i$ 를 이동

$$
\frac{\partial L}{\partial \green{y_i}}=2\sum_j(\green{y_j-y_i})(\blue{p_{j|i}}-\green{q_{j|i}}+\blue{p_{i|j}-\green{q_{i|j}}})
$$

두 sample 간 weight 여도, **어떤 sample이 standard sample 인 지에 따라 차이**가 있다. ($\sigma$ 가 다르기 때문에)

Weight with symmetric 을 갖기 위해, 다음과 같이 정의한다.

$$
\blue{p_{ij}}=\frac{\blue{p_{j|i}}+\blue{p_{i|j}}}{2n}
$$

# t-SNE

Gaussian distribution 은 change to likelihood value 의 gradiant 가 steep(급경사) 한 특성이 있다.

therefore, 일정 거리 이상으로 부터는 value가 매우 작아지는 have crowding problem

먼 거리의 샘플에 대한 weight value 가 have a problem with lose to object( 목적, 의미).

**t-SNE is exchange Gaussian distribution to t distribution, and using them.**

Normal distribution(정규분포) 으로부터 define 되었기 때문에, 이와 매우 유사한 형태의 분포를 가지고 있다.

Number of sample 이 적고, standard deviation(표준편차) of population(모집단) 를 알 수 없을 때, average of population를 assuption 위해 사용한다.

Degree of freedom (자유도) $(n-1)$ 에 따라 shape가 달라지지만, 적은 degree of freedom 에서는 normal distribution 보다 taile shape가 더 thicky하다 .

$$
f(x)=\frac{r(\frac{v+1}{2})}{\sqrt{v\pi}\Gamma(\frac v2)}(1+\frac{x^2}v)^{-\frac{v+1}{2}}, \\ \textrm{where }\Gamma(n)=(n-1)!
$$

Reducted dimenstion 에서의 weight 를 degree of freedom, 1’ t distribution 를 사용

$$
\green{q_{j|i}}=\frac{(1+||\green{y_i-y_j}||^2)^{-1}}{\sum_{k\neq i}(1+||\green{y_i-y_k}||^2)^{-1}}
$$

t-SNE learning에서는 hyperparameter of perplexity 가 필요하다.

설정된 perplexity 를 satisfy 하는 $\sigma_i$ 를 찾아서 algorithm에 사용한다.

$\textrm{Perplexity} = 2^{\textrm{entropy}}=2^{\sum-p_{j|i}\log p_{j|i}}$

Entropy 는 클수록 increase to uncertainty (uniform distribution), 작을수록 decrease to uncertainty (one-hot distribution)

**If set the perplexity to large value**, 모든 sample에 대한 weight 가 비슷해지도록 큰 값의 $\sigma_i$ 를 사용한다.

큰 값의 perplexity 는 dimension reduction 에 관하여 모든 샘플에 대해 increase to influence

In offosite case, nearly sample에 대한 영향력이 매우 커진다.

In normally, It set to 5 to 50.

![Screenshot 2023-03-15 at 12.21.31 PM.png](t-SNE%20fd6407b027c34b2c8001365a1ac309b6/Screenshot_2023-03-15_at_12.21.31_PM.png)