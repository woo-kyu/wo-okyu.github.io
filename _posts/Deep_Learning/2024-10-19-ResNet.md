---
layout: single
title: ResNet Paper Review
toc_label: ResNet Paper Review
categories: 'Deep_Learning'
tags: [Deep Learning, CNN]
author_profile: false
search: true
use_tex: true
---

> ResNet Paper Review

[Original Paper](https://arxiv.org/pdf/1512.03385/)

# 개요

<hr>
<hr>

- 일반적으로 CNN 모델은 층이 더 많아(깊어)질 수록 더 복잡한 특징을 학습 할 수 있다.
- 복잡한 패턴을 이해하기 위해서는 층이 깊어질 수 밖에 없다.
- 그러나, 네트워크의 깊이가 깊을수록 Gradient vanishing(기울기 소실)과 Gradient explosion(기울기 폭발) 문제 가진다.
- 이러한 문제는 모델이 학습을 잘 하지 못하는 결과를 초래한다.
- 이 문제를 해결하기 위해 Residual learning(잔차 학습) 기법을 사용했다.

<br>


# Paper

<hr>
<hr>

## 문제 제시



<br>

## 해결책 제시



<br>





## 용어 정리

### Residual(잔차)[^1]

- Regression(회귀 분석)에서 관측값에서 회귀식에 의한 추정량을 뺀 값
- 잔차 = 관측값 - 예측값


- 오차와의 차이
  - 오차는 모집단의 회귀식에 대한 편차값인데 반해, 잔차는 표본 집단의 회귀식에 대한 편차값.
  - I.e., 오차는 관측값을 톡해 예측한 가정이 실제와 얼마나 부합하는 지의 정도.
  - 잔차는 예측한 가정이 관측값을 얼마나 잘 반영하고 있는지에 대한 의미.

<br>

### [^2]: VLAD


<br>

### [^3]: Fisher Vectors

