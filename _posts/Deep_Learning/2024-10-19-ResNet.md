---
layout: single
title: ResNet Paper Review
toc_label: ResNet Paper Review
categories: 'Deep_Learning'
tags: [Deep Learning, CNN]
author_profile: false
search: true
use_tex: true
---

> ResNet Paper Review

[Original Paper](https://arxiv.org/pdf/1512.03385)

# 개요

<hr>
<hr>

- 일반적으로 CNN 모델은 층이 더 많아(깊어)질 수록 더 복잡한 특징을 학습 할 수 있다.
- 복잡한 패턴을 이해하기 위해서는 층이 깊어질 수 밖에 없다.
- 그러나,  네트워크의 깊이가 깊을수록 Gradient vanishing(기울기 소실)과 Gradient explosion(기울기 폭발) 문제 가진다.
- 이러한 문제는 모델이 학습을 잘 하지 못하는 결과를 초래한다.
- 이 문제를 해결하기 위해 Residual[^1] learning(잔차 학습) 기법을 사용했다.

<br>


# Paper

<hr>
<hr>

## 문제 제시

- 기존 Convolution Neural Network(CNN)은, 레이어가 깊어질수록 더 많은 특징을 추출, 학습할 수 있다.
  - = 네트워크의 깊이가 깊어질 수록 더 많은 특징 레벨을 추출할 수 있다.


- 그러나, 단순히 레이어만 깊게 쌓는 것은 여러 문제를 야기하게 된다.
- 대표적으로, Gradient Vanishing, Gradient Explosion 문제 등이 발생한다.
- 이러한 문제는 VGG 네트워크에서 제안한: normalized initialization, intermediate normalization layers 기법을 통해 일부 해결되었다.
  - To start converging for stochastic gradient descent(SGD) with back-propagation
    - SGD 와 역전파 기법을 통해 네트워크가 성공적으로 converging(수렴, 학습) 할 수 있도록 한다.
    - 중간 정규화 레이어: Batch normalization 과 같은 기법을 사용하여 네트워크의 중간에서 데이터를 정규화
    - Normalized initialization(정규화된 초기화): 신경망의 가중치를 적절히 초기화하여, 네트워크의 학습 초기에 기울기 소실 문제를 줄이는 방법


- 그럼에도 불구하고, 더 깊은 레이어를 사용할 때 수렴(학습)이 가능은 하지만, degradation problem has been exposed. (성능 저하 문제가 발생한다.)
- 네트워크 깊이가 증가함에 따라 정확도가 saturated(포화 상태)에 이르렀다가(더이상 증가하지 않는 시점), 감소하게 된다.
- 이러한 성능 저하 문제는 Over-fitting 문제가 아니며,
- 적절한 깊이를 가진 모델에 더 많은 레이어를 추가할 때, leads to higher training error.

<img width="600" alt="ut" src="https://github.com/user-attachments/assets/8ceadc64-77f6-4d33-9041-f637b335da73">{: .align-center}

- Figure 1. Plain network 에서, 레이어가 깊어질 수록 오류율이 커지는 모습을 볼 수 있다.
- Identity mapping[^2] (항등 함수) 기법을 추가한 더 깊은 네트워크는 레이어가 깊어질 수록 Identity mapping 을 수행하고, 다른 층들은 학습된 shallower model 을 복사하는 알고리즘을 수행하지만,
- 여전히 학습 오류의 발생은 이해할 수 없습니다.

<br>

## 해결책 제시- Residual Learning

이 논문에서는 이러한 문제의 해결 방법으로 Residual Learning(잔차 학습) 기법을 제시한다.
- 


<br>





## Footnote

### 1: Residual(잔차)

> Regression(회귀 분석)에서 관측값에서 회귀식에 의한 추정량을 뺀 값

- 잔차 = 관측값 - 예측값


- 오차와의 차이
  - 오차는 모집단의 회귀식에 대한 편차값인데 반해, 잔차는 표본 집단의 회귀식에 대한 편차값.
  - I.e., 오차는 관측값을 톡해 예측한 가정이 실제와 얼마나 부합하는 지의 정도.
  - 잔차는 예측한 가정이 관측값을 얼마나 잘 반영하고 있는지에 대한 의미.

<br>

### 2: Identity Mapping

> 입력값을 그대로 출력하는 함수, 항등함수
 
> In Deep Learning; 레이어를 거치면서 입력이 변형되지 않고 그대로 출력되는 경우.
>
> 즉, 네트워크의 특정 층이나 shortcut connection 을 통해 입력 데이터를 그대로 다음 층으로 전달하는 것

> **Residual Learning**
>
>잔차 학습에서는 네트워크의 각 층이 복잡한 합수를 직접 학습하는 대신, 잔차(변화의 차이)를 학습하도록 설계. 

- 이때 Identity mapping 는 중요한 역할을 수행하는데, optimal function 이 identity mapping 일 경우, 네트워크는 입력을 그대로 출력하게 만들어야 한다. 
- 항등 함수를 학습하는 것이 목표라면, 네트워크가 여러 비선형 층을 통해 복잡한 변형을 학습하는 것 보다, residual 를 0으로 만드는 것이 더 쉽기 때문.


$f(x) = x$

#### Identity mapping 의 역할

##### Shortcut connection

- 단축 연결
- Residual Network 에서 단축 연결을 사용해 입력을 다음 레이어로 그대로 전달하는 경우, 이 연결이 항등 함수를 수행하게 된다. 
- 입력값이 변형되지 않고 다음 레이어로 전달되므로, 네트워크가 쉽게 최적화 할 수 있게 만든다.

<br>

##### 최적화 단순화

- 네트워크가 항등 함수를 학습할 때, 여러 층을 통과하며 복잡한 변환을 학습하는 것 보다, residual 를 0으로 만들어 항등 함수에 가깝게 만드는 것이 더 쉽다.

<br>


<br>

### Fisher Vectors


<br>

[^1]: Residual(잔차)
[^2]: VLAD
[^3]: Fisher Vectors